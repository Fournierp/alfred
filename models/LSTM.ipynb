{
  "cells": [
    {
      "metadata": {
        "_uuid": "671caa9952527356f8996bc66db57b19935b4a35"
      },
      "cell_type": "markdown",
      "source": "#  Stock Market Dataset\n\n\nIn this kernel, we will design and train the model we will use in our Dash web app. We will develop a Long Short-Term Memory (LSTM) Neural Network and harness its capability to solve problems in time series."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4a134c291e3bb7755859e74f05555191b96355c"
      },
      "cell_type": "code",
      "source": "import os\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nimport datetime\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Input, LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.losses import categorical_crossentropy\nimport tensorflow.keras.backend as K",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dade3db2ca9b776c595dbfd61d4f86d3da51a5b6"
      },
      "cell_type": "markdown",
      "source": "## Pre-Processing\n\nFilter the company's data to use."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68d5bcea37369213b8cdeca619493d533cb85928"
      },
      "cell_type": "code",
      "source": "os.chdir('../input/Data/Stocks/')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0babb279ae5344b9224aa23dba5516040983460f"
      },
      "cell_type": "code",
      "source": "# This function will be used to determine if any value is missing\ndef has_nan(df):\n    return df.isnull().values.any()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2ebda6d366ad6434a513864ea5c02d6948254be"
      },
      "cell_type": "code",
      "source": "# This function will be used to determine if the data for a given company is too sparse\ndef has_few_data(df):\n    if(df.shape[0] < 2000):\n        return True\n    \n    return False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e82b362fb909cff255880d577ec0ce7865c53fee"
      },
      "cell_type": "code",
      "source": "# This function will be used to determine if there is sufficient and complete data about the company stocks.\ndef use_company_data(df):\n    if(has_nan(df)):\n        return False\n    elif(has_few_data(df)):\n        return False\n    else:\n        return True",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c75bd5b496a60a700c1316efa95d1b693a99ac8d"
      },
      "cell_type": "code",
      "source": "files = []\nfor file in os.listdir():\n    try:\n        df = pd.read_csv(file, sep=',')\n        if(use_company_data( df )):\n            files.append(file)\n    except:\n        continue",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bafbf4b80dd8fd109f6f6516d95fe6fb7d581d01"
      },
      "cell_type": "markdown",
      "source": "## Processing\n\nWe will explore two kinds of data types. The first one will be a stock's closing price variation. This is format for data is efficient if we want our model to be accurate for companies with high or low stock prices. Since we have a large database of stock prices, we will also attempt to use the raw stock's closing price."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f77bb9aea56bd3fa6c99e1b11fa20a1c0bfb9cc"
      },
      "cell_type": "code",
      "source": "window_len = 10\n\ndef split_data_variations_in_windows(df):\n    \"\"\"\n    Create series of 10 Closing prices variation and its coresponding 11th price variation.\n    \"\"\"\n    LSTM_inputs = []\n    for i in range(len(df) - window_len):\n        tmp_df = df[i:(i+window_len)].copy()\n        tmp_df = tmp_df/tmp_df.iloc[0] - 1\n\n        LSTM_inputs.append(tmp_df)\n        \n    LSTM_outputs = (df[window_len:].values/df[:-window_len].values)-1\n    LSTM_inputs = [np.array(LSTM_input) for LSTM_input in LSTM_inputs]\n    LSTM_inputs = np.array(LSTM_inputs)\n\n    return LSTM_inputs, LSTM_outputs\n\n\ndef split_data_prices_in_windows(df):\n    \"\"\"\n    Create series of 10 Closing prices and its coresponding 11th price.\n    \"\"\"\n    LSTM_inputs = []\n    for i in range(len(df) - window_len):\n        LSTM_inputs.append(df[i:(i+window_len)])\n        \n    LSTM_outputs = (df['Close'][window_len:].values/df['Close'][:-window_len].values)-1\n    LSTM_inputs = [np.array(LSTM_input) for LSTM_input in LSTM_inputs]\n    LSTM_inputs = np.array(LSTM_inputs)\n    \n    return LSTM_inputs, LSTM_outputs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b7682a95a5c8e16b654930b743d352ba63e4c3e5"
      },
      "cell_type": "markdown",
      "source": "## Model Design\n\nWe will create our model with an Adam Optimizer to accelerate learning. Since this is a regression problem, we will use a Mean Average Error problem."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46e1ed38f1942d2eb9e188f99bbd0d1d390aabed"
      },
      "cell_type": "code",
      "source": "class SGDRScheduler(Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.\n    # Usage\n        ```python\n            schedule = SGDRScheduler(min_lr=1e-5,\n                                     max_lr=1e-2,\n                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n                                     lr_decay=0.9,\n                                     cycle_length=5,\n                                     mult_factor=1.5)\n            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n        ```\n        \n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n        lr_decay: Reduce the max_lr after the completion of each cycle.\n                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n        \n    # References\n        Blog post: jeremyjordan.me/nn-learning-rate\n        Original paper: http://arxiv.org/abs/1608.03983\n    '''\n    \n    def __init__(self,\n                 min_lr,\n                 max_lr,\n                 steps_per_epoch,\n                 lr_decay=1,\n                 cycle_length=10,\n                 mult_factor=2):\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n\n        self.steps_per_epoch = steps_per_epoch\n\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n\n        self.history = {}\n\n    def clr(self):\n        '''Calculate the learning rate.'''\n        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        return lr\n\n    def on_train_begin(self, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.max_lr)\n\n    def on_batch_end(self, batch, logs={}):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.batch_since_restart += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_epoch_end(self, epoch, logs={}):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += self.cycle_length\n            self.max_lr *= self.lr_decay\n            self.best_weights = self.model.get_weights()\n\n    def on_train_end(self, logs={}):\n        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n        self.model.set_weights(self.best_weights)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4f569b5b80849051b83ae4d9086eb3a9e435623"
      },
      "cell_type": "code",
      "source": "class LRFinder(Callback):\n    \n    '''\n    A simple callback for finding the optimal learning rate range for your model + dataset. \n    \n    # Usage\n        ```python\n            lr_finder = LRFinder(min_lr=1e-5, \n                                 max_lr=1e-2, \n                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n                                 epochs=3)\n            model.fit(X_train, Y_train, callbacks=[lr_finder])\n            \n            lr_finder.plot_loss()\n        ```\n    \n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n        \n    # References\n        Blog post: jeremyjordan.me/nn-learning-rate\n        Original paper: https://arxiv.org/abs/1506.01186\n    '''\n    \n    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n        super().__init__()\n        \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch * epochs\n        self.iteration = 0\n        self.history = {}\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        x = self.iteration / self.total_iterations \n        return self.min_lr + (self.max_lr-self.min_lr) * x\n        \n    def on_train_begin(self, logs=None):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.min_lr)\n        \n    def on_batch_end(self, epoch, logs=None):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.iteration += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        K.set_value(self.model.optimizer.lr, self.clr())\n \n    def plot_lr(self):\n        '''Helper function to quickly inspect the learning rate schedule.'''\n        plt.plot(self.history['iterations'], self.history['lr'])\n        plt.yscale('log')\n        plt.xlabel('Iteration')\n        plt.ylabel('Learning rate')\n        \n    def plot_loss(self):\n        '''Helper function to quickly observe the learning rate experiment results.'''\n        plt.plot(self.history['lr'], self.history['loss'])\n        plt.xscale('log')\n        plt.xlabel('Learning rate')\n        plt.ylabel('Loss')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6e4929221a2480691b8ad50433518f456b9d76f"
      },
      "cell_type": "code",
      "source": "def LSTM_model(input_shape, output_shape, neurons, dropout):\n    x = Input(shape=input_shape)\n    hidden = LSTM(2 * neurons, return_sequences=True)(x)\n    hidden = Dropout(dropout)(hidden)\n    hidden = LSTM(neurons, return_sequences=False)(hidden)\n    hidden = Dropout(dropout)(hidden)\n    y = Dense(output_shape, activation='linear')(hidden)\n    return Model(inputs=x, outputs=y)\n\nmodel = LSTM_model((1, 10), 1, 32, 0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0190b3ee5c24d05af31fe056f465660123681a8e"
      },
      "cell_type": "code",
      "source": "class LossHistory(Callback):\n    def __init__(self):\n        self.loss = []\n        self.val_loss = []\n\n    def on_batch_end(self, epoch, logs={}):\n        self.loss.append(logs.get('loss'))\n    \n    def on_epoch_end(self, epoch, logs={}):\n        self.val_loss.append(logs.get('loss'))\n        \ncb = LossHistory()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee25d730799524da5565e32b18279972b6b2946e"
      },
      "cell_type": "code",
      "source": "optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n\nmodel.compile(loss='mse', optimizer=optimizer, metrics=['mse'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47890c4be2a4b142c14c63838f81a42e0277a4da"
      },
      "cell_type": "markdown",
      "source": "### Fine-tuning\n\nLet us firstly look at how an untrained model performes beforehand to have a baseline upon which to improve."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41764aa629d87b8fc9bcfef3ad81d6dc2a844c1f"
      },
      "cell_type": "code",
      "source": "# Randomly select a file (that was approved before hand) and load it.\nfile_nb = random.randint(1, len(files))\ndf = pd.read_csv(files[file_nb], sep=',')[\"Close\"]\n    \n# Process the data\nLSTM_inputs, LSTM_outputs = split_data_variations_in_windows(df)\nLSTM_inputs = np.reshape(LSTM_inputs, (LSTM_inputs.shape[0], 1, LSTM_inputs.shape[1]))\n\n# Evaluate\nmodel.evaluate(LSTM_inputs, LSTM_outputs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17a32751aaef245568d81a27bfe6684e1630799c"
      },
      "cell_type": "code",
      "source": "# find learning rate first\nepoch_size = 20\nbatch_size = 256\n\nlr_finder = LRFinder(min_lr=1e-4, \n                     max_lr=1e-2, \n                     steps_per_epoch=np.ceil(epoch_size/batch_size), \n                     epochs=3)\nmodel.fit(LSTM_inputs, LSTM_outputs, callbacks=[lr_finder])\n\nlr_finder.plot_loss()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f75a694ea4370a41dbc68768063c5b7ea2ab3272"
      },
      "cell_type": "code",
      "source": "schedule = SGDRScheduler(min_lr=1e-3,\n                         max_lr=1e-2,\n                         steps_per_epoch=np.ceil(epoch_size/batch_size),\n                         lr_decay=0.9,\n                         cycle_length= 3,\n                         mult_factor=1.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ecc0ee8f400af72737991b28c97658f34cee589"
      },
      "cell_type": "markdown",
      "source": "# Training\n\nSince every company is unique in that it operates in a specific field, geographic zone, has different sizes and clients, we need to capture as much of that vairance as possible in our model. We will thus train our model using numerous companies rather than a single one."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b320b60dcf2cb7a9106353b43d575bc6b56ff329"
      },
      "cell_type": "code",
      "source": "file_dups = []\n\nfor i in range(10):\n    # Randomly select a file (that was approved before hand) and load it.\n    file_nb = random.randint(1, len(files))\n    if file_nb in file_dups:\n        continue\n    else:\n        file_dups.append(file_nb)\n    df = pd.read_csv(files[file_nb], sep=',')[\"Close\"]\n    \n    # Split the data into train-validation-testing sets (70-10-20).\n    length = df.shape[0]\n    df_train = df[:int(length*0.7)]\n    df_valid = df[int(length*0.7)+1:int(length*0.8)]\n    df_test  = df[int(length*0.8)+1:]\n    \n    # Process the data\n    LSTM_train_inputs, LSTM_train_outputs = split_data_variations_in_windows(df_train)\n    LSTM_valid_inputs, LSTM_valid_outputs = split_data_variations_in_windows(df_valid)\n    LSTM_test_inputs, LSTM_test_outputs = split_data_variations_in_windows(df_test)\n\n    LSTM_train_inputs = np.reshape(LSTM_train_inputs, (LSTM_train_inputs.shape[0], 1, LSTM_train_inputs.shape[1]))\n    LSTM_valid_inputs = np.reshape(LSTM_valid_inputs, (LSTM_valid_inputs.shape[0], 1, LSTM_valid_inputs.shape[1]))\n    LSTM_test_inputs = np.reshape(LSTM_test_inputs, (LSTM_test_inputs.shape[0], 1, LSTM_test_inputs.shape[1]))\n    \n    # Since we want our model to be trained on multiple company stocks, we will only train them for a few epochs.\n    history = model.fit(LSTM_train_inputs, LSTM_train_outputs, \n                        validation_data = (LSTM_valid_inputs, LSTM_valid_outputs),\n                        epochs=5, batch_size=16, verbose=2, callbacks=[cb, schedule])\n# Save the model.\n# model.save('lstm.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0d1eb33897300d5d6e63424ee2bda31cda9620f"
      },
      "cell_type": "code",
      "source": "fig = plt.figure()\n# summarize history for batches\nplt.plot(cb.loss)\nplt.title('Model Train Loss')\nplt.ylabel('Loss')\nplt.xlabel('Batches')\nplt.legend(['loss'], loc='upper left')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ee8cc7d663b6cff1ca0f877afc2303672d854fa"
      },
      "cell_type": "code",
      "source": "fig = plt.figure()\n# summarize history for epochs\nplt.plot(cb.val_loss)\nplt.title('Model Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['acc', 'loss'], loc='upper left')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1fade4058b116caee391ec072821ce8fa39f8698"
      },
      "cell_type": "markdown",
      "source": "## Evaluation\n\nLet us now  look at how our model performs."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d008cecdc478fa00b3b42949b8dfd9c04f540390"
      },
      "cell_type": "code",
      "source": "mse = mean_squared_error(LSTM_outputs, model.predict(LSTM_inputs))\nprint('The Mean Absolute Error is: {}'.format(mse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c85450218b63fbb1ded6f6a566d68b8f8c5fb694"
      },
      "cell_type": "code",
      "source": "x = np.linspace(0, len(LSTM_outputs), len(LSTM_outputs))\ny1 = model.predict(LSTM_inputs)\ny2 = LSTM_outputs\n\nfig, ax = plt.subplots()\nprediction, = ax.plot(x, y1, color='b')\nground_truth, = ax.plot(x, y2, color='g')\n\ndef update(num, x, y1, prediction, y2, ground_truth):\n    prediction.set_data(x[:num], y1[:num])\n    ground_truth.set_data(x[:num], y2[:num])\n    return prediction, ground_truth,\n\nani = animation.FuncAnimation(fig, update, len(x), fargs=[x, y1, prediction, y2, ground_truth],\n                              interval=25, blit=True)\n\nani.save('test.gif', writer=\"imagemagick\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d23efbc3b4716b8a50b185d1020dabac020c091b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}