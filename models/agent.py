from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Input, LSTM
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.models import load_model


class Agent:
	"""
	Reinforcement Learning Agent that learns to trade stocks.
	"""

	def __init__(self, data, prod=False, model_name=""):
        self.data = data
		self.prod = prod			      # Variable to keep track of whether the model is being trained.
		self.memory = deque(maxlen=100)   # Last 100 situations
		self.inventory = []               # Set of all stock purchased prices
        self.t = 0                        # Timer
        self.done = False                 # Boolean to indicate if its the last data point
		self. profit = 0			      # Profit made by the agent

		self.gamma = 0.95

		self.epsilon = 1.                 # Threshold for randomizing decision making during training
		self.epsilon_min = 0.01		      # Minimum threshold
		self.epsilon_decay = 0.995        # Amount by which we reduce the threshold after each batch

		self.model = load_model(model_name) if prod else self.model((1, 10), 1, 16, 0.1)	# Q-Network
		self.batch_size = 32


    def reset(self):
		"""
		Function called upon each epoch to reset the profit and inventory
		"""
		self.inventory = []
        self.t = 0
        self.done = False
		self.profit = 0


    def model(input_shape, output_shape, neurons, dropout):
		"""
		Function that creates the Q-Network.

		:input_shape: number of stocks given to the Network
		:output_shape: number of decisions the model can take
		:neurons: size of the hidden layers
		:dropout: percentage of ignored node inputs

		:return: model
		"""
        x = Input(shape=input_shape)
        hidden = LSTM(2 * neurons, return_sequences=True)(x)
        hidden = Dropout(dropout)(hidden)
        hidden = LSTM(neurons, return_sequences=False)(hidden)
        hidden = Dropout(dropout)(hidden)
        y = Dense(output_shape, activation='linear')(hidden)

        return Model(inputs=x, outputs=y)


	def decision(self, state):
		"""
		Function that determines what the agent does. In training it will randomly
		select a random decision to accelerate learning.

		:state: current number of cash, profit, stock

		:return: optimal or random decision
		"""
		# If the agent is in training and with a random chance.
		if not self.prod and np.random.rand() <= self.epsilon:
			# Return a random decision
			return np.random.randint(0, 4)
		# Else, return the decision chosen by the model
		options = self.model.predict(state)
		return np.argmax(options[0])


	def step(self, action):
        """
        Function that simulates a day of stock trade, where the agent will make a decision.

        :action: action to execute

        :return: state
        """
        reward = 0

        # Given the decision, do the action
        if action == 1:    # Buy
            # Add the stock price at the current time.
            self.inventory.append(self.data.iloc[self.t, :]['Close'])
        elif action == 2 and len(self.inventory) > 0: # Sell
            # Compute the profits generated by subtracting the current stock price with the last bought stock price.
            bought_price = self.inventory.pop(0)
            profit = self.data.iloc[self.t, :]['Close'] - bought_price
            # Treat the profits as reward
            reward += profit
            # Add the current profits to the overall agent's profits
            self.profits += profits

        self.done = True if t == len(self.data) - 1 else False
        # Increment timer
        self.t += 1

        # Activate the reward
        # TODO: find a function that better captures the exponential trend.
        reward = 1 / (1 + math.exp(-reward))

        return self.inventory, reward, self.done # History, reward, last


	def decay(self):
	"""
	Function that decays the threshold that determines when the agen takes random decisions in training.
	"""
	if self.epsilon > self.epsilon_min:
		self.epsilon *= self.epsilon_decay

    def batch():
    	# Get the last (batch_size) elements in memory
    	batch = []
    	l = len(self.memory)
    	for i in range(l - self.batch_size + 1, l):
    		batch.append(self.memory[i])

        return batch


	def learn(self):
		"""
		Function that trains the model.
		"""
        mini_batch = batch()
        error = 0
		# Extract each element in the batch
		for state, decision, reward, next_state, done in mini_batch:
			target = reward
			if not done:
				target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) # Figure out whaaaat that is

			target_f = self.model.predict(state)
			target_f[0][decision] = target
			error += self.model.fit(state, target_f, epochs=1, verbose=0)

		decay()
        return error
